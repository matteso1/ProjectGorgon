# Project Gorgon -- Medusa Head Training Config
# For use with: python scripts/train_medusa_heads.py --config configs/train_heads.yaml

model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
num_heads: 4
max_steps: 500
learning_rate: 1e-4
dataset: "wikitext"
seq_length: 512
head_dtype: "bf16"    # bf16 for A100/H100, fp16 for older GPUs (V100)
batch_size: 4
grad_accum: 4
save_every: 100
warmup_steps: 50
max_grad_norm: 1.0
